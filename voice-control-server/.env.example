# ╔═══════════════════════════════════════════════════════════════════════════╗
# ║          VOICE CONTROL ECOSYSTEM - SERVER CONFIGURATION                   ║
# ║                                                                           ║
# ║  This file contains all configuration options for the voice control      ║
# ║  server. Copy this file to .env and customize values for your setup.     ║
# ║                                                                           ║
# ║  Documentation: docs/setup.md | docs/performance-tuning.md               ║
# ╚═══════════════════════════════════════════════════════════════════════════╝

# ═══════════════════════════════════════════════════════════════════════════
# SERVER CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════
# ENVIRONMENT: Runtime environment mode
#   - development: Enables debug features, verbose logging, hot reload
#   - staging: Pre-production testing with production-like settings
#   - production: Optimized for performance, minimal logging
ENVIRONMENT=development

# DEBUG: Enable detailed error messages and stack traces
#   - true: Show full error details (NEVER use in production)
#   - false: Hide sensitive error information
DEBUG=true

# HOST: Network interface to bind the server
#   - 0.0.0.0: Listen on all available interfaces (required for Docker/remote access)
#   - 127.0.0.1: Listen only on localhost (more secure for local development)
#   - Specific IP: Bind to a particular network interface
HOST=0.0.0.0

# PORT: TCP port for the HTTP/WebSocket server
#   - Default: 8000
#   - Range: 1024-65535 (ports below 1024 require root/admin privileges)
PORT=8000

# ═══════════════════════════════════════════════════════════════════════════
# SECURITY
# ═══════════════════════════════════════════════════════════════════════════
# SECRET_KEY: Cryptographic key for JWT token signing and encryption
#   - CRITICAL: Change this in production!
#   - Generate with: python -c "import secrets; print(secrets.token_hex(32))"
#   - Minimum recommended length: 32 characters
#   - Never commit real secrets to version control
SECRET_KEY=your-secret-key-change-in-production

# ALGORITHM: JWT signing algorithm
#   - HS256: HMAC with SHA-256 (symmetric, fast, good for single-server)
#   - HS384/HS512: Stronger HMAC variants
#   - RS256: RSA with SHA-256 (asymmetric, use for distributed systems)
ALGORITHM=HS256

# ACCESS_TOKEN_EXPIRE_MINUTES: JWT token validity period
#   - Lower values = more secure but require frequent re-authentication
#   - Recommended: 15-60 for high-security, 1440 (24h) for convenience
ACCESS_TOKEN_EXPIRE_MINUTES=30

# ═══════════════════════════════════════════════════════════════════════════
# CORS (Cross-Origin Resource Sharing) SETTINGS
# ═══════════════════════════════════════════════════════════════════════════
# CORS_ORIGINS: Allowed origins for cross-origin requests
#   - JSON array format: ["origin1", "origin2"]
#   - Use ["*"] to allow all origins (NOT recommended for production)
#   - 10.0.2.2 is Android emulator's alias for host machine localhost
#   - Add your React Native app's origin for mobile development
#
# Examples:
#   Development: ["http://localhost:3000", "http://10.0.2.2:3000"]
#   Production: ["https://your-app.com", "https://app.your-domain.com"]
CORS_ORIGINS=["http://localhost:3000", "http://10.0.2.2:3000"]

# ═══════════════════════════════════════════════════════════════════════════
# WEBSOCKET CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════
# WEBSOCKET_MAX_CONNECTIONS: Maximum simultaneous WebSocket connections
#   - Limits memory usage and prevents resource exhaustion
#   - Adjust based on expected concurrent users and server capacity
#   - Each connection uses ~1-5MB depending on audio buffer settings
WEBSOCKET_MAX_CONNECTIONS=10

# WEBSOCKET_PING_INTERVAL: Seconds between ping frames to keep connection alive
#   - Lower values = faster detection of dead connections
#   - Higher values = less network overhead
#   - Recommended: 15-60 seconds
WEBSOCKET_PING_INTERVAL=30

# WEBSOCKET_PING_TIMEOUT: Seconds to wait for pong response before disconnect
#   - Should be less than PING_INTERVAL
#   - Account for network latency (especially mobile networks)
WEBSOCKET_PING_TIMEOUT=10

# ═══════════════════════════════════════════════════════════════════════════
# LOGGING
# ═══════════════════════════════════════════════════════════════════════════
# LOG_LEVEL: Minimum severity level for log messages
#   - DEBUG: All messages including detailed debugging info
#   - INFO: General operational messages (recommended for development)
#   - WARNING: Potential issues that don't stop execution
#   - ERROR: Errors that need attention but app continues
#   - CRITICAL: Severe errors that may crash the application
LOG_LEVEL=INFO

# LOG_FILE: Path to log file (leave empty to log only to console)
#   - Absolute path: /var/log/voice-control/server.log
#   - Relative path: ./storage/logs/server.log
#   - Empty: Console output only (recommended for Docker/containerized)
LOG_FILE=

# ═══════════════════════════════════════════════════════════════════════════
# SPEECH-TO-TEXT (STT) CONFIGURATION - Whisper Model
# ═══════════════════════════════════════════════════════════════════════════
# WHISPER_MODEL: OpenAI Whisper model size for speech recognition
#   ┌──────────────┬────────────┬───────────────┬─────────────────────────────┐
#   │ Model        │ Parameters │ VRAM Required │ Speed vs Accuracy           │
#   ├──────────────┼────────────┼───────────────┼─────────────────────────────┤
#   │ tiny         │ 39M        │ ~1GB          │ Fastest, lowest accuracy    │
#   │ base         │ 74M        │ ~1GB          │ Good balance for dev        │
#   │ small        │ 244M       │ ~2GB          │ Better accuracy             │
#   │ medium       │ 769M       │ ~5GB          │ High accuracy               │
#   │ large        │ 1550M      │ ~10GB         │ Best accuracy (original)    │
#   │ large-v2     │ 1550M      │ ~10GB         │ Improved large model        │
#   │ large-v3     │ 1550M      │ ~10GB         │ Latest, best multilingual   │
#   └──────────────┴────────────┴───────────────┴─────────────────────────────┘
#   Tip: Start with 'base' for development, upgrade for production
WHISPER_MODEL=base

# WHISPER_DEVICE: Compute device for model inference
#   - cpu: Use CPU (slower but universally available)
#   - cuda: Use NVIDIA GPU (requires CUDA toolkit and compatible GPU)
#   - auto: Automatically select best available device
WHISPER_DEVICE=cpu

# WHISPER_COMPUTE_TYPE: Numerical precision for model weights
#   - float32: Full precision (most accurate, highest memory)
#   - float16: Half precision (good balance, requires GPU)
#   - int8: Quantized (fastest, lowest memory, slight accuracy loss)
#   - int8_float16: Mixed precision (GPU only)
#   Note: int8 recommended for CPU, float16 for GPU
WHISPER_COMPUTE_TYPE=int8

# STT_CONFIDENCE_THRESHOLD: Minimum confidence score to accept transcription
#   - Range: 0.0 to 1.0
#   - Lower = accept more uncertain results (may include errors)
#   - Higher = reject low-confidence results (may miss valid speech)
#   - Recommended: 0.6-0.8 depending on noise conditions
STT_CONFIDENCE_THRESHOLD=0.7

# STT_MAX_DURATION: Maximum audio duration in seconds for single transcription
#   - Prevents memory issues with very long recordings
#   - Longer audio is split into chunks automatically
#   - Recommended: 60-300 seconds
STT_MAX_DURATION=300

# ═══════════════════════════════════════════════════════════════════════════
# LANGUAGE MODEL (LLM) CONFIGURATION - Ollama Integration
# ═══════════════════════════════════════════════════════════════════════════
# OLLAMA_BASE_URL: URL of your Ollama server
#   - Local: http://localhost:11434
#   - Docker network: http://ollama:11434
#   - Remote: https://your-ollama-server.com
#   - Ensure Ollama is running: ollama serve
OLLAMA_BASE_URL=http://localhost:11434

# OLLAMA_MODEL: Model to use for intent parsing and response generation
#   ┌──────────────────┬─────────┬────────────────────────────────────────┐
#   │ Model            │ Size    │ Best For                               │
#   ├──────────────────┼─────────┼────────────────────────────────────────┤
#   │ llama2           │ 7B      │ General purpose, good baseline         │
#   │ llama2:13b       │ 13B     │ Better reasoning, more VRAM needed     │
#   │ mistral          │ 7B      │ Fast, efficient, good for commands     │
#   │ codellama        │ 7B      │ Better for code-related commands       │
#   │ phi              │ 2.7B    │ Lightweight, quick responses           │
#   │ neural-chat      │ 7B      │ Conversational, natural responses      │
#   │ gemma3n:e4b      │ 4B      │ Google's efficient model, fast         │
#   └──────────────────┴─────────┴────────────────────────────────────────┘
#   Pull model first: ollama pull gemma3n:e4b
OLLAMA_MODEL=gemma3n:e4b

# OLLAMA_TIMEOUT: Maximum seconds to wait for LLM response
#   - Increase for slower hardware or larger models
#   - Decrease for snappier UX (may cut off complex responses)
OLLAMA_TIMEOUT=30

# LLM_MAX_TOKENS: Maximum tokens in LLM response
#   - Voice commands typically need 50-150 tokens
#   - Increase for more detailed explanations
#   - 1 token ≈ 4 characters or 0.75 words
LLM_MAX_TOKENS=1024

# LLM_TEMPERATURE: Randomness in LLM responses
#   - 0.0: Deterministic, always same response (good for commands)
#   - 0.3-0.5: Slightly varied but focused (recommended)
#   - 0.7-1.0: More creative and varied responses
#   - >1.0: Very random, may produce inconsistent results
LLM_TEMPERATURE=0.5

# ═══════════════════════════════════════════════════════════════════════════
# MCP (MODEL CONTEXT PROTOCOL) CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════
# MCP_TIMEOUT: Maximum seconds to wait for MCP tool execution
#   - Some operations (file search, browser automation) may take longer
#   - Increase if you experience timeout errors on complex operations
MCP_TIMEOUT=10

# MCP_MAX_RESULTS: Maximum results returned from MCP queries
#   - Limits response size for search operations
#   - Higher values = more complete results but slower response
MCP_MAX_RESULTS=100

# ═══════════════════════════════════════════════════════════════════════════
# AUDIO PROCESSING
# ═══════════════════════════════════════════════════════════════════════════
# AUDIO_SAMPLE_RATE: Audio sampling frequency in Hz
#   - 16000: Standard for speech recognition (Whisper's native rate)
#   - 44100: CD quality (higher quality but larger files, gets resampled)
#   - 48000: Professional audio (unnecessary for speech)
#   Note: Higher rates increase bandwidth and processing time
AUDIO_SAMPLE_RATE=16000

# AUDIO_CHANNELS: Number of audio channels
#   - 1: Mono (recommended for speech - smaller files, faster processing)
#   - 2: Stereo (unnecessary for voice commands)
AUDIO_CHANNELS=1

# AUDIO_BIT_DEPTH: Bits per audio sample
#   - 16: Standard quality (recommended)
#   - 24: Higher dynamic range (overkill for speech)
#   - 32: Professional audio (unnecessary)
AUDIO_BIT_DEPTH=16

# AUDIO_CHUNK_SIZE: Bytes per audio chunk for streaming
#   - Smaller = lower latency but more network overhead
#   - Larger = higher latency but more efficient
#   - Recommended: 1024-8192 bytes
#   - Formula: samples_per_chunk = CHUNK_SIZE / (BIT_DEPTH/8) / CHANNELS
AUDIO_CHUNK_SIZE=4096

# AUDIO_MAX_BUFFER_SIZE: Maximum audio buffer in bytes
#   - Prevents memory exhaustion from very long recordings
#   - 10MB = ~10 minutes of 16-bit mono 16kHz audio
#   - Adjust based on expected maximum recording length
AUDIO_MAX_BUFFER_SIZE=10485760

# ═══════════════════════════════════════════════════════════════════════════
# DATABASE (OPTIONAL)
# ═══════════════════════════════════════════════════════════════════════════
# DATABASE_URL: Database connection string for persistent storage
#   - SQLite (simple, no server needed):
#     DATABASE_URL=sqlite:///./storage/voice_control.db
#   - PostgreSQL (recommended for production):
#     DATABASE_URL=postgresql://username:password@localhost:5432/voice_control_db
#   - MySQL:
#     DATABASE_URL=mysql://username:password@localhost:3306/voice_control_db
#
# Leave commented out to use default SQLite or in-memory storage
# DATABASE_URL=postgresql://username:password@localhost/voice_control_db
# DATABASE_URL=sqlite:///./storage/voice_control.db

# ═══════════════════════════════════════════════════════════════════════════
# CACHE (OPTIONAL)
# ═══════════════════════════════════════════════════════════════════════════
# REDIS_URL: Redis connection for caching and session storage
#   - Improves performance for frequent lookups
#   - Enables horizontal scaling with shared session state
#   - Format: redis://[[username:]password@]host[:port][/database]
#
# Leave commented out to use in-memory caching
# REDIS_URL=redis://localhost:6379/0
# REDIS_URL=redis://:password@localhost:6379/0

# ═══════════════════════════════════════════════════════════════════════════
# FILE STORAGE
# ═══════════════════════════════════════════════════════════════════════════
# STORAGE_PATH: Base directory for file storage
#   - Audio recordings, cache files, and logs are stored here
#   - Use absolute path in production: /var/data/voice-control
#   - Ensure write permissions for the application user
STORAGE_PATH=./storage

# MAX_FILE_SIZE: Maximum upload file size in bytes
#   - 104857600 = 100MB
#   - Limits single audio file uploads
#   - Increase for longer recordings or batch uploads
MAX_FILE_SIZE=104857600

# ═══════════════════════════════════════════════════════════════════════════
# PERFORMANCE
# ═══════════════════════════════════════════════════════════════════════════
# MAX_WORKERS: Thread pool size for blocking operations
#   - Handles CPU-bound tasks like audio processing
#   - Recommended: Number of CPU cores (or cores - 1 for responsiveness)
#   - Too high = context switching overhead, too low = underutilization
MAX_WORKERS=4

# ASYNC_SEMAPHORE_LIMIT: Maximum concurrent async operations
#   - Prevents overwhelming external services
#   - Limits parallel LLM/STT requests
#   - Increase if you have powerful hardware and fast network
ASYNC_SEMAPHORE_LIMIT=100

# REQUEST_TIMEOUT: Maximum seconds for any single request
#   - Includes audio upload + transcription + LLM processing
#   - Increase for slower hardware or complex operations
REQUEST_TIMEOUT=60

# RATE_LIMIT_REQUESTS: Maximum requests per minute per client
#   - Prevents abuse and ensures fair resource distribution
#   - 0 = no rate limiting (not recommended for production)
RATE_LIMIT_REQUESTS=60

# ═══════════════════════════════════════════════════════════════════════════
# MONITORING
# ═══════════════════════════════════════════════════════════════════════════
# ENABLE_METRICS: Enable Prometheus metrics endpoint
#   - Exposes /metrics endpoint for monitoring systems
#   - Tracks request latency, error rates, resource usage
ENABLE_METRICS=true

# METRICS_PORT: Port for Prometheus metrics server
#   - Separate from main API port for security isolation
#   - Typically not exposed publicly
METRICS_PORT=9090

# HEALTH_CHECK_INTERVAL: Seconds between internal health checks
#   - Monitors STT, LLM, and MCP service connectivity
#   - Lower values = faster problem detection but more overhead
HEALTH_CHECK_INTERVAL=30

# ═══════════════════════════════════════════════════════════════════════════
# DEVELOPMENT OPTIONS
# ═══════════════════════════════════════════════════════════════════════════
# HOT_RELOAD: Enable automatic server restart on code changes
#   - true: Restart server when Python files change (development)
#   - false: Manual restart required (production)
HOT_RELOAD=true

# AUTO_RELOAD_MODELS: Automatically reload ML models on file change
#   - Useful when fine-tuning or swapping models
#   - Can cause brief service interruption during reload
AUTO_RELOAD_MODELS=false

# MOCK_SERVICES: Use mock implementations for external services
#   - Enables testing without Ollama, Whisper, or MCP servers
#   - Returns simulated responses for development/testing
MOCK_SERVICES=false

# ═══════════════════════════════════════════════════════════════════════════
# FEATURE FLAGS
# ═══════════════════════════════════════════════════════════════════════════
# Enable/disable individual features without code changes
# Useful for gradual rollouts, A/B testing, or troubleshooting

# ENABLE_STT: Enable speech-to-text transcription
#   - Requires Whisper model to be loaded
ENABLE_STT=true

# ENABLE_LLM: Enable language model for intent parsing
#   - Requires Ollama server to be running
ENABLE_LLM=true

# ENABLE_MCP: Enable Model Context Protocol integrations
#   - Required for Chrome DevTools and Windows automation
ENABLE_MCP=true

# ENABLE_AUDIO_PROCESSING: Enable audio preprocessing pipeline
#   - Includes noise reduction, normalization, format conversion
ENABLE_AUDIO_PROCESSING=true

# ENABLE_TRANSCRIPTION: Enable real-time transcription display
#   - Shows live transcription in client apps
ENABLE_TRANSCRIPTION=true

# ENABLE_STREAMING: Enable WebSocket streaming for audio
#   - Required for real-time voice control
#   - Disable to use only REST API for batch processing
ENABLE_STREAMING=true
